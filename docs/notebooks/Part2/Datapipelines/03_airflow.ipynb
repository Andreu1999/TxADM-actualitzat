{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Airflow\n",
    "\n",
    "Installing Apache Airflow\n",
    "- https://airflow.apache.org/\n",
    "\n",
    "Vamos a instalar nuestro Apache Airflow en un Ubuntu usando multipass.\n",
    "\n",
    "\n",
    "En la doc de Airflow vemos que hay dos formas: en local o con docker (asignatura de Cloud Computing) \n",
    "- Versión local: https://airflow.apache.org/docs/apache-airflow/stable/start/local.html , \n",
    "\n",
    "Seguimos los pasos correspondientes sobre nuestro Ubuntu:\n",
    "\n",
    "```bash\n",
    "# Airflow needs a home. `~/airflow` is the default, but you can put it\n",
    "# somewhere else if you prefer (optional)\n",
    "export AIRFLOW_HOME=~/airflow\n",
    "\n",
    "# Install Airflow using the constraints file\n",
    "AIRFLOW_VERSION=2.3.2\n",
    "PYTHON_VERSION=\"$(python3 --version | cut -d \" \" -f 2 | cut -d \".\" -f 1-2)\"\n",
    "# For example: 3.7\n",
    "CONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\n",
    "# For example: https://raw.githubusercontent.com/apache/airflow/constraints-2.3.2/constraints-3.7.txt\n",
    "pip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n",
    "\n",
    "# The Standalone command will initialise the database, make a user,\n",
    "# and start all components for you.\n",
    "airflow standalone\n",
    "\n",
    "# Visit localhost:8080 in the browser and use the admin account details\n",
    "# shown on the terminal to login.\n",
    "# Enable the example_bash_operator dag in the home page\n",
    "```\n",
    "\n",
    "### Reflexiones en este punto\n",
    "\n",
    "- ¿Como podemos visitar la página localhost:8080 en nuestro navegador? \n",
    "- ¿Esto que significa?\n",
    "- ¿Qué tipo de servicio o uso tenemos con Airflow?\n",
    "- ¿Dónde está ejecutandose?\n",
    "\n",
    "\n",
    "### SSH \n",
    "\n",
    "```bash\n",
    "ssh usuario@IP\n",
    "\n",
    "ssh ubuntu@192.168.64.8\n",
    "#que ha ocurrido?\n",
    "\n",
    "ssh -L 8080:192.168.64.8:8080 airflow@192.168.64.8\n",
    "\n",
    "```\n",
    "\n",
    "Averiguar la IP de nuestras máquinas con multipass\n",
    "```bash\n",
    "% multipass list \n",
    "Name                    State             IPv4             Image\n",
    "master                  Running           192.168.64.8     Ubuntu 20.04 LTS\n",
    "                                          10.1.219.64\n",
    "```\n",
    "\n",
    "Pasos necesarios para facilitar una conexión remota basada en passwords a nuestro equipo:\n",
    "- En la MV de Ubuntu, cambiaremos un \"no\" por un \"yes\":  <br/>\n",
    "  ```bash\n",
    "  sudo nano /etc/ssh/sshd_config <br/>\n",
    "  ```\n",
    "  <img src=\"images/nanoSSHD.png\"/>\n",
    "  <br/>\n",
    "- Reiniciamos el servicio (¿Qué es un servicio/daemon?)\n",
    "  ```bash\n",
    "  sudo systemctl restart sshd  \n",
    "  ```\n",
    "\n",
    "- Desde nuestra máquina Host ya podemos conectarnos:  <br/> ssh -L 8080:192.168.64.8:8080 airflow@192.168.64.8<br/> \n",
    "- Y en cualquier navegador del HOST podemos acceder a la url pertinente: *localhost:8080* <br/> \n",
    " <img src=\"images/airflowcaptura.png\"/>\n",
    "\n",
    "- Finalmente, activamos el ejemplo: *example_bash_operator* . ¿Qué información nos ofrecen las diferentes pestañas?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creando nuestro primer DAG "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a realizar ciertos puntos del tutorial:\n",
    "- https://airflow.apache.org/docs/apache-airflow/stable/tutorial.html\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as rnd\n",
    "import time\n",
    "\n",
    "def my_task1():\n",
    "    # Esta función es muy compleja, que obtiene un valor después de muchisímo tiempo de ejecución\n",
    "    try:\n",
    "        time.sleep(5)\n",
    "        value = rnd.random()\n",
    "        return {\"value1\":value}\n",
    "    except:\n",
    "        none\n",
    "\n",
    "def my_task2():\n",
    "    # Esta función es muy compleja; pero no tarda tanto.\n",
    "    try:\n",
    "        value = rnd.random()\n",
    "        time.sleep(8)\n",
    "        return {\"value2\":value}\n",
    "    except:\n",
    "        none\n",
    "\n",
    "\n",
    "def my_task_max(value1, value2):\n",
    "    # Esta función muestra como combinar los resultados de otras tareas predecesoras\n",
    "    valueMax = max(value1[\"value1\"],value2[\"value2\"])\n",
    "    with open(\"/tmp/mydata.csv\",\"a+\") as f:\n",
    "        f.write(\"%.4f,\"%valueMax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "default_args = {\n",
    "    'depends_on_past': False,\n",
    "    'email': ['airflow@example.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "\n",
    "}\n",
    "\n",
    "dag = DAG('4miPrimerDAG', \n",
    "     default_args=default_args,\n",
    "     start_date = datetime(2019,1,1),#datetime.now()-timedelta(minutes=1),\n",
    "     schedule_interval = timedelta(minutes=2) \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = PythonOperator(dag=dag,\n",
    "        task_id='my_task1',\n",
    "        python_callable=my_task1)\n",
    "\n",
    "t2 = PythonOperator(dag=dag,\n",
    "        task_id='my_task2',\n",
    "        python_callable=my_task2)\n",
    "\n",
    "\n",
    "t3 = PythonOperator(dag=dag,\n",
    "        task_id='my_MAX',\n",
    "        op_kwargs={'value1': t1.output, 'value2': t2.output},\n",
    "        python_callable=my_task_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [t1, t2] >> t3\n",
    "\n",
    "t1.set_downstream(t3)\n",
    "t2.set_downstream(t3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Todo junto\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "import random as rnd\n",
    "import time\n",
    "\n",
    "def my_task1():\n",
    "    # Esta función es muy compleja, que obtiene un valor después de muchisímo tiempo de ejecución\n",
    "    try:\n",
    "        time.sleep(5)\n",
    "        value = rnd.random()\n",
    "        return {\"value1\":value}\n",
    "    except:\n",
    "        none\n",
    "\n",
    "def my_task2():\n",
    "    # Esta función es muy compleja; pero no tarda tanto.\n",
    "    try:\n",
    "        value = rnd.random()\n",
    "        time.sleep(8)\n",
    "        return {\"value2\":value}\n",
    "    except:\n",
    "        none\n",
    "\n",
    "\n",
    "def my_task_max(value1, value2):\n",
    "    # Esta función muestra como combinar los resultados de otras tareas predecesoras\n",
    "    valueMax = max(value1[\"value1\"],value2[\"value2\"])\n",
    "    with open(\"/tmp/mydata.csv\",\"a+\") as f:\n",
    "        f.write(\"%.4f,\"%valueMax)\n",
    "\n",
    "default_args = {\n",
    "    'depends_on_past': False,\n",
    "    'email': ['airflow@example.com'],\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "\n",
    "}\n",
    "\n",
    "dag = DAG('4miPrimerDAG', \n",
    "     default_args=default_args,\n",
    "     start_date = datetime(2019,1,1),#datetime.now()-timedelta(minutes=1),\n",
    "     schedule_interval = timedelta(minutes=2) \n",
    "    )\n",
    "\n",
    "t1 = PythonOperator(dag=dag,\n",
    "        task_id='my_task1',\n",
    "        python_callable=my_task1)\n",
    "\n",
    "t2 = PythonOperator(dag=dag,\n",
    "        task_id='my_task2',\n",
    "        python_callable=my_task2)\n",
    "\n",
    "\n",
    "t3 = PythonOperator(dag=dag,\n",
    "        task_id='my_MAX',\n",
    "        op_kwargs={'value1': t1.output, 'value2': t2.output},\n",
    "        python_callable=my_task_max)\n",
    "\n",
    "\n",
    "# [t1, t2] >> t3\n",
    "\n",
    "t1.set_downstream(t3)\n",
    "t2.set_downstream(t3)\n",
    "\n",
    "# Recordatorio de algunos comandos\n",
    "# python3 mydag.py\n",
    "# airflow db init\n",
    "\n",
    "# airflow tasks list miPrimerDAG\n",
    "# airflow tasks test miPrimerDAG my_task1\n",
    "# airflow dags trigger miPrimerDAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## References\n",
    "- Data Pipelines with Apache Airflow. Manning Publications Bas P. Harenslak, Julian Rutger de Ruiter. 2021\n",
    "- https://betterdatascience.com/apache-airflow-run-tasks-in-parallel/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
